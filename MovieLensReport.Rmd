---
title: "HarvardX PH125.9x Data Science Capstone: MovieLens Project"
author: "Daoxia Ding"
date: "7/1/2022"
output:
  pdf_document:
    toc: true
    number_sections: true
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, include=FALSE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################
# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(caret)
library(data.table)
```

```{r Code From Class, eval=TRUE, include=FALSE, cache=TRUE}
# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip
dl <- tempfile()
download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
# movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                            title = as.character(title),
#                                            genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>%
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)
# rm(): Remove Objects from Memory in R Programming
rm(dl, ratings, movies, test_index, temp, movielens, removed)

##########################################################
```

# Introduction

The MovieLens data is provided as part of HarvardX PH125.9x Data Science Capstone course. The reduced size version was also used throughout the textbook by Rafael Irizarry. It includes more than 10 million ratings user submitted for movies.  
The data set can be found through the link below.  
https://files.grouplens.org/datasets/movielens/ml-10m.zip  
The target of this project is to develop and train a recommendation system model based on the MovieLens 10M dataset. The Residual Mean Square Error (RMSE) is used to evaluate the loss of the algorithm. The ultimate target of RMSE is to reach below 0.86490.  
Due to the large size of the data, existing R lm() model is not used due to computational limits on the laptop. Instead, we are computing it without using lm() in R. We will also use regularization in the model to penalize large estimates that are formed using small sample sizes.  
This report, following the course requirements, will explain the process to explore the data, clean the data, split the data for training and testing, identify four effects to be included in the linear model, implement regularization, and eventually apply the model on the validation set and conclude with the RMSE result.
\pagebreak

# Data Exploration and Analysis

```{r check missing libraries, include=FALSE}
if(!require(rafalib)) install.packages("rafalib", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("rafalib", repos = "http://cran.us.r-project.org")
if(!require(formatR)) install.packages("rafalib", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("rafalib", repos = "http://cran.us.r-project.org")
library(rafalib)
library(lubridate)
library(formatR)
library(kableExtra)
```

## General Overview
We review the dimension of the dataset "edx",
```{r}
dim(edx)
```
as well as the dimension of the dataset "validation":
```{r}
dim(validation)
```

A look at the head and summary of the dataset "edx" reveals that there are 6 columns. timestamp column would need to be converted to date format. We may need to split the genre column to individual categories instead of a single string.
```{r}
head(edx)
summary(edx)
```
Below is the head and summary of the dataset "validation". It's very similar to "edx".
```{r}
head(validation) %>%
  kable() %>%
  kable_styling(position = "center", latex_options = c("hold_position", "scale_down"))
summary(validation)
```

The "edx" dataset includes close to 70,000 unique users and over 10,000 unique movies:
```{r}
# number of unique users and unique movies
edx %>% summarize(n_users = n_distinct(userId), n_movies = n_distinct(movieId))
```

The data sets are pretty clean since analysis shows no missing value in "edx",
```{r}
sapply(edx, function(x) sum(is.na(x)))
```
And no missing value in "validation" as well. Therefore no action is needed on treating missing values.
```{r}
sapply(validation, function(x) sum(is.na(x)))
```

Imagine if we set row as each unique users, column as each unique movie. Then the whole question/target becomes to fill in the blanks in this matrix.
Below visualization shows how sparse this matrix is with 500 users and 500 movies.  
```{r, fig.align="center", out.width="75%"}
users <- sample(unique(edx$userId), 500)
rafalib::mypar()
edx %>% filter(userId %in% users) %>% 
  select(userId, movieId, rating) %>%
  mutate(rating = 1) %>%
  spread(movieId, rating) %>% 
  select(sample(ncol(.), 500)) %>% 
  as.matrix() %>% 
  t(.) %>%
  image(1:500, 1:500,. , xlab="Movies", ylab="Users")
```

The movie ratings are between 0.5 and 5.0 with 0.5 increments. 
As you can see below, in general, half star ratings are less common than whole star ratings.
```{r, fig.align="center", out.width="75%"}
# Ratings - histogram
edx %>% 
  ggplot(aes(rating)) + 
  geom_histogram(bins = 10, color = "black") + 
  ggtitle("Ratings Histogram")
```
Here is the same information but in table view.
```{r}
# Ratings - table view - sort by number of ratings
edx %>% 
  count(rating) %>% 
  arrange(desc(n))
```

\pagebreak
## Movie Effects
Let's take a look at some distributions relevant to the movies.
Below shows Movie Distribution by Number of Ratings. We can see some movies get rated more than others.
```{r, fig.align="center", out.width="75%"}
# Movie Distribution by Number of Ratings
# (insight) some movies get rated more than others
edx %>% 
  count(movieId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  xlab("Number of Ratings") + ylab("Number of Movies") +
  scale_x_log10() + 
  ggtitle("Movie Distribution by Number of Ratings")
```
Below shows Movie Distribution by Average Ratings. We can see some movies get rated higher than others.
```{r, fig.align="center", out.width="75%"}
# Movie Distribution by Average Ratings
# (insight) some movies get rated higher than others
edx %>% 
  group_by(movieId) %>% 
  summarize (avg_rating = mean(rating)) %>% 
  ggplot(aes(avg_rating)) + 
  geom_histogram(bins = 30, color = "black") + 
  xlab("Average Rating") + ylab("Number of Movies") + 
  ggtitle("Movie Distribution by Average Ratings")
# (insight) above shows in linear model, we should consider movie effects
```
In summary, above indicates in a linear model, we should consider movie effects.

\pagebreak
## User Effects
Let's now take a look at distributions relevant to the users.
Below shows User Distribution by Number of Ratings. It clearly shows some users are more active than others at rating movies.
```{r, fig.align="center", out.width="75%"}
edx %>% 
  count(userId) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  xlab("Number of Ratings") + ylab("Number of Users") +
  scale_x_log10() + 
  ggtitle("User Distribution by Number of Ratings")
```
Below shows Users Distribution by Average Ratings. It clearly show some users give higher ratings than other users.
```{r, fig.align="center", out.width="75%"}
edx %>% 
  group_by(userId) %>% 
  summarize (avg_rating = mean(rating)) %>% 
  ggplot(aes(avg_rating)) + 
  geom_histogram(bins = 30, color = "black") + 
  xlab("Average Rating") + ylab("Number of Users") + 
  ggtitle("Users Distribution by Average Ratings")
```
In summary, above indicates in a linear model, we should consider user effects as well.

\pagebreak
## Genre Effects
Now let's do similar visualizations on the distributions relevant to the genres.
Below shows Genre Distribution by Number of Ratings. Some genres receive more ratings than others.
```{r, fig.align="center", out.width="75%"}
edx %>% 
  count(genres) %>% 
  ggplot(aes(n)) + 
  geom_histogram(bins = 30, color = "black") + 
  xlab("Number of Ratings") + ylab("Number of Genre") +
  scale_x_log10() + 
  ggtitle("Genre Distribution by Number of Ratings")
```
Below shows Genre Distribution by Average Ratings. Some genres receive higher ratings than others.
```{r, fig.align="center", out.width="75%"}
edx %>% 
  group_by(genres) %>% 
  summarize (avg_rating = mean(rating)) %>% 
  ggplot(aes(avg_rating)) + 
  geom_histogram(bins = 30, color = "black") + 
  xlab("Average Rating") + ylab("Number of Genres") + 
  ggtitle("Genres Distribution by Average Ratings")
```
In summary, above indicates in a linear model, we should consider genre effects as well.  
But do we need to split the genre column into individual genres? Let's split the genre column and look at same distributions analysis.  
Below shows Number of Ratings by Genre (splitted).   
```{r, cache=TRUE, fig.align="center", out.width="75%"}
edx_generes_collaps <- edx %>%
  separate_rows(genres, sep = "\\|", convert = TRUE)
edx_generes_collaps %>% 
  ggplot(aes(genres)) + 
  geom_bar() +
  # (note) angle: angle of text, from 0 to 360. hjust: horizontal justification, from 0 to 1 (left justified to right justified).
  theme(axis.text.x = element_text(angle = 90,hjust = 1, vjust = 0.5)) + 
  xlab("Genre") + ylab("Number of Ratings") +
  ggtitle("Number of Ratings by Genre (splitted)")
```
Below shows Average Rating by Genre (splitted) .
```{r, cache=TRUE, fig.align="center", out.width="75%"}
edx_generes_collaps %>%
  group_by(genres) %>%
  summarize (avg_rating = mean(rating)) %>%
  ggplot(aes(genres,avg_rating)) + 
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle = 90,hjust = 1, vjust = 0.5)) + 
  xlab("Genre") + ylab("Average Rating") +
  ggtitle("Average Rating by Genre (splitted)")
```
Above reinforced the idea that in a linear model, we should definitely consider genre effects.  
Also, due to less variability on Average Rating by the individual genres, I decided to use the original combined genre column without splitting it.  

\pagebreak
## Data Cleaning
So far our data exploration covered user, movie, and genres. In order to further explore the effects of rating timestamps, we need to clean the data.  
Below code is to convert timestamp to Date type, extract movie release year from movie title, and calculate the year between movie release and rating time stamp.  
```{r, cache=TRUE, echo = TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=55)}
edx <- edx %>%
  mutate(title_temp = str_trim(title), date = as.Date(as_datetime(timestamp))) %>%
  # year_after_release: the year between movie release and rating time stamp
  extract(title_temp, c("title_temp", "releaseyear"), regex = "^(.*) \\(([0-9 \\-]*)\\)$", remove = FALSE) %>%
  mutate(releaseyear = if_else(str_length(releaseyear) > 4,
                               as.integer(str_split(releaseyear, "-", simplify = TRUE)[1]),
                               as.integer(releaseyear))) %>%
  mutate(year_after_release = isoyear(date)-releaseyear) %>%
  select(-title_temp, -timestamp)
head(edx) %>%
  kable() %>%
  kable_styling(position = "center", latex_options = c("hold_position", "scale_down"))
# do the same to validation dataset (I'm not adding/deleting any rows.)
validation <- validation %>%
  mutate(title_temp = str_trim(title), date = as.Date(as_datetime(timestamp))) %>%
  extract(title_temp, c("title_temp", "releaseyear"), regex = "^(.*) \\(([0-9 \\-]*)\\)$", remove = FALSE) %>%
  mutate(releaseyear = if_else(str_length(releaseyear) > 4,
                               as.integer(str_split(releaseyear, "-", simplify = TRUE)[1]),
                               as.integer(releaseyear))) %>%
  mutate(year_after_release = isoyear(date)-releaseyear) %>%
  select(-title_temp, -timestamp)
head(validation) %>%
  kable() %>%
  kable_styling(position = "center", latex_options = c("hold_position", "scale_down"))
```

\pagebreak
## Rate Time Effects
Now we can take a look at possible Rate Time Effects.  
Let's first look at Weekly Number of Ratings. It turns out there is not much insight.
```{r, message=FALSE, fig.align="center", out.width="75%"}
# Weekly Number of Ratings
edx %>% 
  mutate(date = round_date(date, unit="week")) %>% 
  group_by(date) %>%
  summarize(wk_no_rating = n()) %>%
  ggplot(aes(date, wk_no_rating)) +
  geom_line() +
  geom_smooth() +
  xlab("Date") + ylab("Weekly Number of Ratings") +
  ggtitle("Weekly Number of Ratings")
```
We then look at Weekly Rating Average. Again, not much insight.
```{r, message=FALSE, fig.align="center", out.width="75%"}
# Weekly Rating Average
edx %>% 
  mutate(date = round_date(date, unit="week")) %>% 
  group_by(date) %>%
  summarize(wk_avg = mean(rating)) %>%
  ggplot(aes(date, wk_avg)) +
  geom_line() +
  geom_smooth() +
  xlab("Date") + ylab("Weekly Rating Average") +
  ggtitle("Weekly Rating Average")
```
Instead of using rating time, let's look at the rating time in comparison to the movie release year - "Years After Movie Release".  
Below we look at Number of Ratings by Years After Movie Release. As you can see below, the number of ratings for a new released movie topped in the first 5 years, and then reduce over the time.  
```{r, message=FALSE, fig.align="center", out.width="75%"}
# Number of Ratings by Years After Movie Release
edx %>% 
  group_by(year_after_release) %>%
  summarize(wk_no_rating = n()) %>%
  ggplot(aes(year_after_release, wk_no_rating)) +
  geom_line() +
  xlab("Years After Movie Release") + ylab("Number of Ratings") +
  ggtitle("Number of Ratings by Years After Movie Release")
```
Let's also take a look at Average Rating by Years After Movie Release. 
```{r, message=FALSE, fig.align="center", out.width="75%"}
# Average Rating by Years After Movie Release
edx %>% 
  group_by(year_after_release) %>%
  summarize(wk_avg = mean(rating)) %>%
  ggplot(aes(year_after_release, wk_avg)) +
  geom_line() +
  geom_smooth() +
  xlab("Years After Movie Release") + ylab("Average Rating") +
  ggtitle("Average Rating by Years After Movie Release")
```
Since the online movie rating website came with Web 2.0 and is something less than 30 years old, I'm focusing on the trend within 30 years after movie release. During that period of time, the average rating tends to go up as time passes. Therefore we should consider rating time effects.

\pagebreak
# Model Building and Methods

I'm using the code below to split data into training and testing sets in order to prepare for model building.
```{r, echo = TRUE, message=FALSE, results = 'hide', tidy=TRUE, tidy.opts=list(width.cutoff=55), warning=FALSE}
# Split data sets and prepare for model building
# set.seed(755) # if using R 3.5 or earlier
set.seed(755, sample.kind = "Rounding") # if using R 3.6 or later
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train_set <- edx[-test_index,]
test_set <- edx[test_index,]
# To make sure we don’t include movies, users, genres, year_after_release in the test set that do not appear in the training set, we remove these entries using the semi_join function:
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId") %>%
  semi_join(train_set, by = "genres") %>%
  semi_join(train_set, by = "year_after_release")
# Add rows removed from test_set back into train_set
test_set_removed <- anti_join(edx[test_index,], test_set)
train_set <- rbind(train_set, test_set_removed)
```
We will use RMSE as the loss function, here we write a custom function for that:
```{r, echo = TRUE}
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings - predicted_ratings)^2))
}
```

## Model#1: Just the average
This is the simplest model we start with. In this model, we predict all movies with just the mean. The formula is
$$Y_{u,i}=\mu+\epsilon_{u,i}$$
where $\mu$ is one "true" rating for all movies. $\epsilon$ is independent errors sampled from the same distribution centered at zero, $i$ is movie, $u$ is user.    
```{r, echo = TRUE}
mu_hat <- mean(train_set$rating)
model_1_rmse <- RMSE(test_set$rating, mu_hat)
# create a results table.
rmse_results <- tibble(method = "Just the average", RMSE = model_1_rmse)
rmse_results %>% knitr::kable()
```
The RMSE result shows our typical error is larger than one star, which is not good prediction.

## Model#2.1: Movie Effect Model (b_i)
We then take into account the Movie Effects. The formula is
$$Y_{u,i}=\mu+b_i+\epsilon_{u,i}$$
where $b_i$ is average rating effect of the movie $i$.  
By plotting a chart, we see it proves each movie's $b_i$ (bias) varies substantially.
```{r, echo = TRUE, fig.align="center", out.width="75%", tidy=TRUE, tidy.opts=list(width.cutoff=55)}
# Since lm() is too slow and probably crash your laptop as complexity grows, we will calculate using code below.
mu <- mean(train_set$rating)
movie_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = mean(rating - mu))
# by plotting a chart, we see it proves each movie's b_i (bias) varies substantially
movie_avgs %>% qplot(b_i, geom ="histogram", bins = 10, data = ., color = I("black"))
# Add the result to the results table
predicted_ratings <- mu + test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  pull(b_i)
model_2_1_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results, tibble(method="Movie Effect Model", RMSE = model_2_1_rmse ))
rmse_results %>% knitr::kable()
```
The RMSE result shows improvements compared to "Just the average" model.  

## Model#2.2: Movie + User Effects Model (b_i + b_u)
Let's add User Effects as well. The formula is
$$Y_{u,i}=\mu+b_i+b_u+\epsilon_{u,i}$$
where $b_u$ is average rating effect of the user $u$.  
By plotting a chart of the average rating for user u for those that have rated 100 or more movies. It proves that there is substantial variability across users as well.
```{r, echo = TRUE, fig.align="center", out.width="75%", tidy=TRUE, tidy.opts=list(width.cutoff=55), warning=FALSE}
train_set %>% 
  group_by(userId) %>% 
  filter(n()>=100) %>%
  summarize(b_u = mean(rating)) %>% 
  ggplot(aes(b_u)) + 
  geom_histogram(bins = 30, color = "black")
user_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_i))
# Add the result to the results table
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  mutate(pred = mu + b_i + b_u) %>%
  pull(pred)
model_2_2_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results, data_frame(method="Movie + User Effects Model", RMSE = model_2_2_rmse))
rmse_results %>% knitr::kable()
```
The RMSE result shows improvements compared to "Movie Effect Model" model.   

## Model#2.3: Movie + User + Genre Effects Model (b_i + b_u + b_g)
Next, we are adding Genre Effects. The formula is
$$Y_{u,i}=\mu+b_i+b_u+b_g+\epsilon_{u,i}$$
where $b_g$ is average rating effect of the genre $g$.  
By plotting a chart of the average rating for genre g for those that have 1000 or more movies. It proves that there is substantial variability across genres as well.
```{r, echo = TRUE, fig.align="center", out.width="75%", tidy=TRUE, tidy.opts=list(width.cutoff=55), warning=FALSE}
train_set %>% 
  group_by(genres) %>% 
  filter(n()>=1000) %>%
  summarize(b_g = mean(rating)) %>% 
  ggplot(aes(b_g)) + 
  geom_histogram(bins = 30, color = "black")
genre_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - mu - b_i - b_u))
# Add the result to the results table
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres') %>%
  mutate(pred = mu + b_i + b_u + b_g) %>%
  pull(pred)
model_2_3_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method="Movie + User + Genre Effects Model", 
                                     RMSE = model_2_3_rmse))
rmse_results %>% knitr::kable()
```
The RMSE result shows improvements compared to "Movie + User Effects Model" model.   

## Model#2.4: Movie + User + Genre + Rate Time Effects Model (b_i + b_u + b_g + b_t)
Let's add the last effects to our model - Rate Time Effects. The formula is
$$Y_{u,i}=\mu+b_i+b_u+b_g+b_t+\epsilon_{u,i}$$
where $b_t$ is average rating effect of the rate time $t$.  
```{r, echo = TRUE, fig.align="center", out.width="75%", tidy=TRUE, tidy.opts=list(width.cutoff=55), warning=FALSE}
rate_time_avgs <- train_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres') %>%
  group_by(year_after_release) %>%
  summarize(b_t = mean(rating - mu - b_i - b_u - b_g))
# Add the result to the results table
predicted_ratings <- test_set %>% 
  left_join(movie_avgs, by='movieId') %>%
  left_join(user_avgs, by='userId') %>%
  left_join(genre_avgs, by='genres') %>%
  left_join(rate_time_avgs, by='year_after_release') %>%
  mutate(pred = mu + b_i + b_u + b_g + b_t) %>%
  pull(pred)
model_2_4_rmse <- RMSE(predicted_ratings, test_set$rating)
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method="Movie + User + Genre + Rate Time Effects Model", 
                                     RMSE = model_2_4_rmse))
rmse_results %>% knitr::kable()
```
The RMSE result shows best performance so far compared to all previous models.   

\pagebreak
## Model#3.1: Regularized Movie Effect Model
Regularization permits us to penalize large estimates that are formed using small sample sizes. This should help us further improve the model.  
Let's try it on "Movie Effect Model".  
```{r, cache=TRUE, echo = TRUE, fig.align="center", out.width="75%", tidy=TRUE, tidy.opts=list(width.cutoff=55), warning=FALSE}
lambdas <- seq(0, 10, 0.25)
just_the_sum <- train_set %>% 
  group_by(movieId) %>% 
  summarize(s = sum(rating - mu), n_i = n())
rmses <- sapply(lambdas, function(l){
  predicted_ratings <- test_set %>% 
    left_join(just_the_sum, by='movieId') %>% 
    mutate(b_i = s/(n_i+l)) %>%
    mutate(pred = mu + b_i) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
})
# lambda_b_i=2.25
qplot(lambdas, rmses)  
lambda_b_i <- lambdas[which.min(rmses)]
model_3_1_rmse <- min(rmses)
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method="Regularized Movie Effect Model", 
                                     RMSE = model_3_1_rmse))
rmse_results %>% knitr::kable()
```
The RMSE result did show improvements compared to the original "Movie Effects Model" model.    

## Model#3.2: Regularized Movie + User Effect Model
```{r, cache=TRUE, echo = TRUE, fig.align="center", out.width="75%", tidy=TRUE, tidy.opts=list(width.cutoff=55), warning=FALSE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i)/(n()+l))
  predicted_ratings <- test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    mutate(pred = mu + b_i + b_u) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
})
# lambda_b_u=4.75
qplot(lambdas, rmses)  
lambda_b_u <- lambdas[which.min(rmses)]
model_3_2_rmse <- min(rmses)
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method="Regularized Movie + User Effect Model", 
                                                   RMSE = model_3_2_rmse))
rmse_results %>% knitr::kable()
```
The RMSE result did show improvements compared to the original "Movie  + User Effects Model" model. 

## Model#3.3: Regularized Movie + User + Genre Effect Model
```{r, cache=TRUE, echo = TRUE, fig.align="center", out.width="75%", tidy=TRUE, tidy.opts=list(width.cutoff=55), warning=FALSE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i)/(n()+l))
  b_g <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - mu - b_i - b_u)/(n()+l))
  predicted_ratings <- test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    mutate(pred = mu + b_i + b_u + b_g) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
})
# lambda_b_g=4.75
qplot(lambdas, rmses)  
lambda_b_g <- lambdas[which.min(rmses)]
model_3_3_rmse <- min(rmses)
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method="Regularized Movie + User + Genre Effect Model", 
                                                   RMSE = model_3_3_rmse))
rmse_results %>% knitr::kable()
```
The RMSE result did show improvements compared to the original "Movie  + User + Genre Effects Model" model. 

## Model#3.4: Regularized Movie + User + Genre + Rate Time Effect Model
```{r, cache=TRUE, echo = TRUE, fig.align="center", out.width="75%", tidy=TRUE, tidy.opts=list(width.cutoff=55), warning=FALSE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  mu <- mean(train_set$rating)
  b_i <- train_set %>%
    group_by(movieId) %>%
    summarize(b_i = sum(rating - mu)/(n()+l))
  b_u <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mu - b_i)/(n()+l))
  b_g <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - mu - b_i - b_u)/(n()+l))
  b_t <- train_set %>% 
    left_join(b_i, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    group_by(year_after_release) %>%
    summarize(b_t = sum(rating - mu - b_i - b_u - b_g)/(n()+l))
  predicted_ratings <- test_set %>% 
    left_join(b_i, by = "movieId") %>%
    left_join(b_u, by = "userId") %>%
    left_join(b_g, by = "genres") %>%
    left_join(b_t, by = "year_after_release") %>%
    mutate(pred = mu + b_i + b_u + b_g + b_t) %>%
    pull(pred)
  return(RMSE(predicted_ratings, test_set$rating))
})
# lambda_b_t=5
qplot(lambdas, rmses)  
lambda_b_t <- lambdas[which.min(rmses)]
model_3_4_rmse <- min(rmses)
rmse_results <- bind_rows(rmse_results, 
                          data_frame(method="Regularized Movie + User + Genre + Rate Time Effect Model", 
                                     RMSE = model_3_4_rmse))
rmse_results %>% knitr::kable()
```
The RMSE result is so far the best, even better than the original "Movie  + User + Genre + Rate Time Effects Model" model. 

## Final Model
Based on the lowest RMSE, we are choosing "Regularized Movie + User + Genre + Rate Time Effect Model" as our final model.  

\pagebreak
# Results
Let's take a quick recap on all the lambda parameters we picked:  
```{r, echo = FALSE}
kable(data.frame(lambda_b_i=lambda_b_i, lambda_b_u=lambda_b_u, lambda_b_g=lambda_b_g, lambda_b_t=lambda_b_t))
# lambda_b_i
# lambda_b_u
# lambda_b_g
# lambda_b_t
```
The final model is developed based from the "edx" data set. The validation set is not used at all.
```{r, echo = TRUE, fig.align="center", out.width="75%", tidy=TRUE, tidy.opts=list(width.cutoff=55), warning=FALSE}
# use the final model developed based on edx data set
mu <- mean(train_set$rating)
movie_reg_avgs <- train_set %>% 
  group_by(movieId) %>% 
  summarize(b_i = sum(rating - mu)/(n()+lambda_b_i))
user_reg_avgs <- train_set %>%
  left_join(movie_reg_avgs, by="movieId") %>%
  group_by(userId) %>% 
  summarize(b_u = sum(rating - b_i - mu)/(n()+lambda_b_u)) 
genre_reg_avgs <- train_set %>%
  left_join(movie_reg_avgs, by="movieId") %>%
  left_join(user_reg_avgs, by="userId") %>%
  group_by(genres) %>% 
  summarize(b_g = sum(rating - b_i - mu - b_u)/(n()+lambda_b_g)) 
ratetime_reg_avgs <- train_set %>%
  left_join(movie_reg_avgs, by="movieId") %>%
  left_join(user_reg_avgs, by="userId") %>%
  left_join(genre_reg_avgs, by="genres") %>%
  group_by(year_after_release) %>% 
  summarize(b_t = sum(rating - b_i - mu - b_u - b_g)/(n()+lambda_b_t)) 
# implement the model on validation set, and see result
# RMSE=0.8648385, which has achieved ultimate target: RMSE < 0.86490 
predicted_ratings <- validation %>% 
  left_join(movie_reg_avgs, by = "movieId") %>%
  left_join(user_reg_avgs, by = "userId") %>%
  left_join(genre_reg_avgs, by = "genres") %>%
  left_join(ratetime_reg_avgs, by = "year_after_release") %>%
  mutate(pred = mu + b_i + b_u + b_g + b_t) %>%
  pull(pred)
model_final_rmse <- RMSE(predicted_ratings, validation$rating)
RMSE(predicted_ratings, validation$rating)
```
The final RMSE we get by applying our final model on the "validation" dateset is `r round(model_final_rmse, 7)`. This proves that we have achieved ultimate target: RMSE < 0.86490.

\pagebreak
# Conclusion

After using only "edx" dataset to test different models, we ended up constructed the "Regularized Movie + User + Genre + Rate Time Effect Model". The final model takes into consideration the effects from movie, user, genre and rate time. Due to sparsity of the data, we added regularization to further improve the model performance.  
After applying our final model on the "validation" dateset (previously unused), We have successfully achieved RMSE of `r round(model_final_rmse, 7)`. This is beyond the ultimate target of RMSE below 0.86490.   

Even though we have reached the target set by this course, we need to realize there are limitations on this final model. One example is that it assumes movie, user, genre and rate time are all independent, which is most likely not the case in real world. 

Looking forward, I would be interested to see more information regarding the users (such as location, age, gender, etc.) and the movies (such as box office sales, investment, director, actors, country, etc.) in order to improve further the model performance. Matrix factorization would be another method to consider as well. 

